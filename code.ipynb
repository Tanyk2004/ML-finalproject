{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Final Project\n",
    "\n",
    "### Tanay, Vishal, Nikshita, Garv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "\n",
    "The machine learning problem that we are trying to solve is to predict whether a passanger aboard the Spaceship Titanic was transported to an alternate dimension during its collision with a spacetime anaomaly. This can be described as a binary classification probelm with the target variable \"Transported\" indicating True or False. This problem matters because by coming up with an optimal solution will help provide insight on improving the safety engineering features of future space voyages, thus reducing the risks of space travel. The results of our predicitive model could be used to further enhance safety measures and improve risk management. Being able to help the goal of designing more efficient and reliable space travel methods. By predicting the transported variable we can help identify the potential causes and thus make targeted adjustments in ship design and passenger safety.\n",
    "\n",
    "### Description of the Dataset and Attributes\n",
    "`PassengerId` - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "\n",
    "`HomePlanet` - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "\n",
    "`CryoSleep` - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "\n",
    "`Cabin` - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "\n",
    "`Destination` - The planet the passenger will be debarking to.\n",
    "\n",
    "`Age` - The age of the passenger.\n",
    "\n",
    "`VIP` - Whether the passenger has paid for special VIP service during the voyage.\n",
    "\n",
    "`RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck` - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "\n",
    "`Name` - The first and last names of the passenger.\n",
    "\n",
    "`Transported` - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn matplotlib numpy pandas tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from pandas import DataFrame\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many NaNs there are \n",
    "rows_with_nan = df.isnull().any(axis=1).sum()\n",
    "rows_without_nan = len(df) - rows_with_nan\n",
    "\n",
    "print(f\"Rows with NaN: {rows_with_nan}\")\n",
    "print(f\"Rows without NaN: {rows_without_nan}\")\n",
    "\n",
    "print(\"Columns with NaNs: \", df.isnull().any())\n",
    "\n",
    "# Checking what the data looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to see the distributions of the variables we wanna impute (mean or median)\n",
    "variables = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Age']\n",
    "\n",
    "# Configure subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create histograms for each variable\n",
    "for i, var in enumerate(variables):\n",
    "    axes[i].hist(df[var].dropna(), bins=30)\n",
    "    axes[i].set_title(f'Distribution of {var}')\n",
    "    axes[i].set_xlabel(var)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rationale: Based on the results of our data exploration, we have decided to process the data in the following ways: Initially, we planned on dropping all 2087 records with NaNs, but we later revisited this as the accuracies we were getting from our models was not as high as we thought it could be, and we felt that more data could help with this. Thus, we decided to impute numerical features with the mean for the feature, and categorical features with the the value that best fits the probability distribution. We did not impute binary features as we felt that since there were only 2 possible values, imputation was risky and could drastically impact the output if the value was off. In the end, we dropped 786 records with NaNs, leaving 7907 records left, We will revisit this if necessary. We will one hot encode the HomePlanet and Destination fields as they are categorical. We will drop the Name field since it is unique (or near unique) for each passenger, and it seems unlikely it could provide useful information. As the Cabin field essentially has three pieces of information (deck, number, and side), we have elected to break it down into three fields. Similarly, as the Passenger_Id field has two pieces of information (group number and passenger number), we will break it down into two fields. We will one hot encode the deck as it has only a handful of options. We will convert the new side feature from P or S into True or False. For all numeric features (RoomService, FoodCourt, ShoppingMall, Spa, VRDeck Age, Room, Group, PassengerNumber), we will standardize the values so that  we can conduct PCA. Lastly, we will conduct PCA on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df = df\n",
    "\n",
    "print(\"Columns with NaNs before imputation: \", imputed_df.columns[imputed_df.isnull().any()].tolist())\n",
    "\n",
    "# fill NaNs in HomePlanet with random values based on distribution\n",
    "value_counts = imputed_df['HomePlanet'].value_counts(normalize=True)\n",
    "imputed_df[\"HomePlanet\"] = imputed_df[\"HomePlanet\"].fillna(lambda: np.random.choice(value_counts.index, p=value_counts.values))\n",
    "\n",
    "# fill NaNs in Destination with random values based on distribution\n",
    "value_counts = imputed_df['Destination'].value_counts(normalize=True)\n",
    "imputed_df[\"Destination\"] = imputed_df[\"Destination\"].fillna(lambda: np.random.choice(value_counts.index, p=value_counts.values))\n",
    "\n",
    "# Impute RoomService with the mean\n",
    "imputed_df[\"RoomService\"] = imputed_df[\"RoomService\"].fillna(imputed_df[\"RoomService\"].median())\n",
    "\n",
    "# Impute FoodCourt with the mean\n",
    "imputed_df[\"FoodCourt\"] = imputed_df[\"FoodCourt\"].fillna(imputed_df[\"FoodCourt\"].median())\n",
    "\n",
    "# Impute ShoppingMall with the mean\n",
    "imputed_df[\"ShoppingMall\"] = imputed_df[\"ShoppingMall\"].fillna(imputed_df[\"ShoppingMall\"].median())\n",
    "\n",
    "# Impute Spa with the mean\n",
    "imputed_df[\"Spa\"] = imputed_df[\"Spa\"].fillna(imputed_df[\"Spa\"].median())\n",
    "\n",
    "# Impute VRDeck with the mean\n",
    "imputed_df[\"VRDeck\"] = imputed_df[\"VRDeck\"].fillna(imputed_df[\"VRDeck\"].median())\n",
    "\n",
    "# Impute Age with the mean\n",
    "imputed_df['Age'] = imputed_df['Age'].fillna(imputed_df['Age'].mean())\n",
    "\n",
    "# drop NaNs\n",
    "processed_features = imputed_df.dropna()\n",
    "\n",
    "# Drop Name values\n",
    "imputed_df = imputed_df.drop(columns=['Name'])\n",
    "\n",
    "# Split Cabin values into three columns\n",
    "imputed_df[[\"Deck\", \"Room\", \"Side\"]] = imputed_df['Cabin'].str.split(\"/\", expand=True)\n",
    "imputed_df = imputed_df.drop(columns=['Cabin'])\n",
    "imputed_df.head()\n",
    "\n",
    "# Split Passenger values into two columns\n",
    "imputed_df[[\"Group\", \"Passenger_Number\"]] = imputed_df['PassengerId'].str.split(\"_\", expand=True)\n",
    "imputed_df = imputed_df.drop(columns=['PassengerId'])\n",
    "imputed_df.head()\n",
    "\n",
    "print(\"Columns with NaNs after imputation: \", imputed_df.columns[imputed_df.isnull().any()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = imputed_df\n",
    "\n",
    "# One hot encode the HomePlanet\n",
    "processed_df = pd.get_dummies(processed_df, columns=[\"HomePlanet\"])\n",
    "\n",
    "# One hot encode the DestinationPlanet\n",
    "processed_df = pd.get_dummies(processed_df, columns=[\"Destination\"])\n",
    "\n",
    "# One hot encode the Deck\n",
    "processed_df = pd.get_dummies(processed_df, columns=[\"Deck\"])\n",
    "\n",
    "# convert Side to T or F\n",
    "processed_df[\"Side\"] = processed_df['Side'].map({'P': True, 'S' : False})\n",
    "\n",
    "# normalizing numeric features\n",
    "scaler = StandardScaler()\n",
    "processed_df['RoomService'] = scaler.fit_transform(processed_df[['RoomService']])\n",
    "processed_df['FoodCourt'] = scaler.fit_transform(processed_df[['FoodCourt']])\n",
    "processed_df['ShoppingMall'] = scaler.fit_transform(processed_df[['ShoppingMall']])\n",
    "processed_df['Spa'] = scaler.fit_transform(processed_df[['Spa']])\n",
    "processed_df['VRDeck'] = scaler.fit_transform(processed_df[['VRDeck']])\n",
    "processed_df['Age'] = scaler.fit_transform(processed_df[['Age']])\n",
    "processed_df['Group'] = scaler.fit_transform(processed_df[['Group']])\n",
    "processed_df['Passenger_Number'] = scaler.fit_transform(processed_df[['Passenger_Number']])\n",
    "\n",
    "rows_with_nan = processed_df.isnull().any(axis=1).sum()\n",
    "rows_without_nan = len(processed_df) - rows_with_nan\n",
    "\n",
    "print(f\"Rows with NaN: {rows_with_nan}\")\n",
    "print(f\"Rows without NaN: {len(processed_df) - rows_with_nan}\")\n",
    "\n",
    "processed_df = processed_df.dropna()\n",
    "\n",
    "rows_with_nan = processed_df.isnull().any(axis=1).sum()\n",
    "rows_without_nan = len(processed_df) - rows_with_nan\n",
    "\n",
    "print(f\"Rows with NaN: {rows_with_nan}\")\n",
    "print(f\"Rows without NaN: {len(processed_df) - rows_with_nan}\")\n",
    "\n",
    "labels = processed_df[\"Transported\"]\n",
    "processed_features = processed_df.drop(labels = \"Transported\", axis=\"columns\")\n",
    "\n",
    "pca = PCA(n_components=0.95, svd_solver='full')\n",
    "pca_data = pca.fit_transform(processed_features)\n",
    "pca_df = DataFrame(pca_data)\n",
    "\n",
    "print(\"Original data shape:\", processed_features.shape)\n",
    "print(\"Transformed data shape:\", pca_df.shape)\n",
    "\n",
    "processed_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to plot\n",
    "features_to_plot = [\n",
    "    'RoomService', 'FoodCourt', 'ShoppingMall', \n",
    "    'Spa', 'VRDeck', 'Age', \n",
    "    'Group', 'Passenger_Number'\n",
    "]\n",
    "\n",
    "# Create subplots for histograms and boxplots\n",
    "fig, axes = plt.subplots(nrows=8, ncols=2, figsize=(14, 24))  # 8 rows, 2 columns\n",
    "axes = axes.flatten()  # Flatten axes for easier iteration\n",
    "\n",
    "# Plot both histograms and boxplots\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    # Plot histogram\n",
    "    axes[2 * i].hist(processed_features[feature], bins=30, color='blue', alpha=0.7)\n",
    "    axes[2 * i].set_title(f\"Histogram of {feature}\")\n",
    "    axes[2 * i].set_xlabel(\"Normalized Value\")\n",
    "    axes[2 * i].set_ylabel(\"Frequency\")\n",
    "    axes[2 * i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Plot boxplot\n",
    "    axes[2 * i + 1].boxplot(processed_features[feature], vert=False, patch_artist=True)\n",
    "    axes[2 * i + 1].set_title(f\"Boxplot of {feature}\")\n",
    "    axes[2 * i + 1].set_xlabel(\"Normalized Value\")\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = processed_df.corr()\n",
    "\n",
    "sorted_indices = correlation.abs().mean().sort_values(ascending=False).index  # Sort by average absolute correlation\n",
    "sorted_correlation = correlation.loc[sorted_indices, sorted_indices]\n",
    "\n",
    "ax = sns.heatmap(sorted_correlation, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "\n",
    "# Display all attributes\n",
    "ax.set_xticks(np.arange(len(sorted_correlation.columns)) + 0.5, minor=False)\n",
    "ax.set_yticks(np.arange(len(sorted_correlation.index)) + 0.5, minor=False)\n",
    "\n",
    "# Assign proper labels to all ticks\n",
    "ax.set_xticklabels(sorted_correlation.columns, fontsize=6, rotation=45, ha=\"right\")  # Rotate x-axis labels\n",
    "ax.set_yticklabels(sorted_correlation.index, fontsize=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Age\"]\n",
    "n_plots = len(attributes)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(5 * n_plots, 5), sharey=False)\n",
    "\n",
    "for i, attribute in enumerate(attributes):\n",
    "    sns.boxplot(\n",
    "        data=imputed_df,\n",
    "        x=\"HomePlanet\",\n",
    "        y=attribute,\n",
    "        palette=\"Set2\",\n",
    "        ax=axes[i],\n",
    "        hue=\"HomePlanet\"\n",
    "    )\n",
    "    axes[i].set_title(f\"{attribute} vs HomePlanet\")\n",
    "    axes[i].set_xlabel(\"HomePlanet\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought there would be a correlation between the HomePlanet and a few different attributes, but what I found that the data is all over the place. The only real outliers that I found was that Europa tended to spend a lot more on the food court and spa more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30],\n",
    "    'min_samples_leaf': [5, 10, 15, 20, 25, 30],\n",
    "    'max_features': [5, 10, 15, 20, 25, 30],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# runs the nested cross validation\n",
    "acc = cross_val_score(GridSearchCV(clf, param_grid, cv=5), X=pca_df, y=labels, cv=10)\n",
    "print(acc.mean() * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ss = StandardScaler()\n",
    "pca = PCA()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', ss),\n",
    "    ('pca', pca),\n",
    "    ('knn', knn),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(1, 11)),\n",
    "    'knn__n_neighbors': list(range(1, 10))\n",
    "}\n",
    "\n",
    "inner_cv = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "acc = cross_val_score(inner_cv, X=pca_df, y=labels, cv=5)\n",
    "\n",
    "print(acc.mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import sklearn as sk\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', ss),\n",
    "    ('pca', pca),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "params_grid = {\n",
    "    'pca__n_components': list(range(5, 19)),\n",
    "    'svc__kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "inner_cv = GridSearchCV(pipeline, params_grid, cv=5, scoring='accuracy')\n",
    "label_preds = cross_val_predict(inner_cv, X=pca_df, y=labels, cv=10)\n",
    "\n",
    "class_report = sk.metrics.classification_report(labels, label_preds)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "# Accuracy is in the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_df, labels, test_size=0.2)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    # Output layer for binary classification\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Print Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
