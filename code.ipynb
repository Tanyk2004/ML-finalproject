{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Final Project\n",
    "\n",
    "### Tanay, Vishal, Nikshita, Garv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.21.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.1.5)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (3.20.1)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.13.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.62.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (0.42.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (4.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: mock>=2.0.0 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow) (6.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard<1.14.0,>=1.13.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\garvk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow) (3.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn matplotlib numpy pandas tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from pandas import DataFrame\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "labels = df['Transported']\n",
    "# features = df.drop(columns=['Transported'])\n",
    "features = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "\n",
    "# Checking how many NaNs there are \n",
    "rows_with_nan = df.isnull().any(axis=1).sum()\n",
    "rows_without_nan = len(df) - rows_with_nan\n",
    "\n",
    "print(f\"Rows with NaN: {rows_with_nan}\")\n",
    "print(f\"Rows without NaN: {rows_without_nan}\")\n",
    "\n",
    "# Checking what the data looks like\n",
    "df.head()\n",
    "\n",
    "''' Based on the results of our data exploration, we have decided to process the \n",
    "    data in the following ways:\n",
    "    We will drop the 2087 records with NaNs as there would still be 6606 records \n",
    "    left, which seems sufficient to train a model with. We will revisit this if \n",
    "    necessary.\n",
    "    We will one hot encode the HomePlanet and Destination fields as they are \n",
    "    categorical. \n",
    "    We will drop the Name field since it is unique (or near unique) for each passenger,\n",
    "    and it seems unlikely it could provide useful information.\n",
    "    As the Cabin field essentially has three pieces of information (deck, number,\n",
    "    and side), we have elected to break it down into three fields.\n",
    "    Similarly, as the Passenger_Id field has two pieces of information (group number\n",
    "    and passenger number), we will break it down into two fields.\n",
    "    We will one hot encode the deck as it has only a handful of options.\n",
    "    We will convert the new side feature from P or S into True or False.\n",
    "    For all numeric features (RoomService, FoodCourt, ShoppingMall, Spa, VRDeck\n",
    "    Age, Room, Group, PassengerNumber), we will standardize the values so that \n",
    "    we can conduct PCA.\n",
    "    Lastly, we will conduct PCA on the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"Columns with NaNs: \", features.isnull().any())\n",
    "\n",
    "# fill NaNs in HomePlanet with random values based on distribution\n",
    "value_counts = features['HomePlanet'].value_counts(normalize=True)\n",
    "features[\"HomePlanet\"] = features[\"HomePlanet\"].fillna(lambda: \n",
    "                                                       np.random.choice(value_counts.index, p=value_counts.values)) \n",
    "\n",
    "# fill NaNs in Destination with random values based on distribution\n",
    "value_counts = features['Destination'].value_counts(normalize=True)\n",
    "features[\"Destination\"] = features[\"Destination\"].fillna(lambda: \n",
    "                                                         np.random.choice(value_counts.index, p=value_counts.values))\n",
    "\n",
    "# Impute RoomService with the mean\n",
    "features[\"RoomService\"] = features[\"RoomService\"].fillna(features[\"RoomService\"].mean())\n",
    "\n",
    "# Impute FoodCourt with the mean\n",
    "features[\"FoodCourt\"] = features[\"FoodCourt\"].fillna(features[\"FoodCourt\"].mean())\n",
    "\n",
    "# Impute ShoppingMall with the mean\n",
    "features[\"ShoppingMall\"] = features[\"ShoppingMall\"].fillna(features[\"ShoppingMall\"].mean())\n",
    "\n",
    "# Impute Spa with the mean\n",
    "features[\"Spa\"] = features[\"Spa\"].fillna(features[\"Spa\"].mean())\n",
    "\n",
    "# Impute VRDeck with the mean\n",
    "features[\"VRDeck\"] = features[\"VRDeck\"].fillna(features[\"VRDeck\"].mean())\n",
    "\n",
    "# Impute Age with the mean\n",
    "features['Age'] = features['Age'].fillna(features['Age'].mean())\n",
    "\n",
    "print(\"Columns with NaNs: \", features.columns[features.isnull().any()].tolist())\n",
    "\n",
    "# drop NaNs\n",
    "processed_features = features.dropna()\n",
    "\n",
    "print(\"Number of Dropped Records: \", len(features) - len(processed_features))\n",
    "print(\"Number of Records Left: \", len(processed_features))\n",
    "\n",
    "# Drop Name values\n",
    "processed_features = processed_features.drop(columns=['Name'])\n",
    "\n",
    "# Split Cabin values into three columns\n",
    "processed_features[[\"Deck\", \"Room\", \"Side\"]] = processed_features['Cabin'].str.split(\"/\", expand=True)\n",
    "processed_features = processed_features.drop(columns=['Cabin'])\n",
    "processed_features.head()\n",
    "\n",
    "# Split Passenger values into two columns\n",
    "processed_features[[\"Group\", \"Passenger_Number\"]] = processed_features['PassengerId'].str.split(\"_\", expand=True)\n",
    "processed_features = processed_features.drop(columns=['PassengerId'])\n",
    "processed_features.head()\n",
    "\n",
    "# One hot encode the HomePlanet\n",
    "processed_features = pd.get_dummies(processed_features, columns=[\"HomePlanet\"])\n",
    "\n",
    "# One hot encode the DestinationPlanet\n",
    "processed_features = pd.get_dummies(processed_features, columns=[\"Destination\"])\n",
    "\n",
    "# One hot encode the Deck\n",
    "processed_features = pd.get_dummies(processed_features, columns=[\"Deck\"])\n",
    "\n",
    "# convert Side to T or F\n",
    "processed_features[\"Side\"] = processed_features['Side'].map({'P': True, 'S' : False})\n",
    "\n",
    "# normalizing numeric features\n",
    "scaler = StandardScaler()\n",
    "processed_features['RoomService'] = scaler.fit_transform(processed_features[['RoomService']])\n",
    "processed_features['FoodCourt'] = scaler.fit_transform(processed_features[['FoodCourt']])\n",
    "processed_features['ShoppingMall'] = scaler.fit_transform(processed_features[['ShoppingMall']])\n",
    "processed_features['Spa'] = scaler.fit_transform(processed_features[['Spa']])\n",
    "processed_features['VRDeck'] = scaler.fit_transform(processed_features[['VRDeck']])\n",
    "processed_features['Age'] = scaler.fit_transform(processed_features[['Age']])\n",
    "processed_features['Group'] = scaler.fit_transform(processed_features[['Group']])\n",
    "processed_features['Passenger_Number'] = scaler.fit_transform(processed_features[['Passenger_Number']])\n",
    "\n",
    "labels = processed_features[\"Transported\"]\n",
    "processed_features = processed_features.drop(labels = \"Transported\",axis=\"columns\")\n",
    "\n",
    "pca = PCA(n_components=0.95, svd_solver='full')\n",
    "pca_data = pca.fit_transform(processed_features)\n",
    "pca_df = DataFrame(pca_data)\n",
    "\n",
    "print(\"Original data shape:\", processed_features.shape)\n",
    "print(\"Transformed data shape:\", pca_df.shape)\n",
    "\n",
    "processed_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_leaf': [5, 10, 15, 20],\n",
    "    'max_features': [5, 10, 15],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# runs the nested cross validation\n",
    "acc = cross_val_score(GridSearchCV(clf, param_grid, cv=5), X=processed_features, y=labels, cv=10)\n",
    "print(acc.mean() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Pipeline + Model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ss = StandardScaler()\n",
    "pca = PCA()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', ss),\n",
    "    ('pca', pca),\n",
    "    ('knn', knn),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(1, 11)),\n",
    "    'knn__n_neighbors': list(range(1, 10))\n",
    "}\n",
    "\n",
    "inner_cv = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "acc = cross_val_score(inner_cv, X=processed_features, y=labels, cv=5)\n",
    "\n",
    "print(acc.mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a support vector machine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import sklearn as sk\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', ss),\n",
    "    ('pca', pca),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "params_grid = {\n",
    "    'pca__n_components': list(range(5, 19)),\n",
    "    'svc__kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "inner_cv = GridSearchCV(pipeline, params_grid, cv=5, scoring='accuracy')\n",
    "label_preds = cross_val_predict(inner_cv, X=processed_features, y=labels, cv=10)\n",
    "\n",
    "class_report = sk.metrics.classification_report(labels, label_preds)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "# Accuracy is in the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Necessary Imports\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Necessary Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_df, labels, test_size=0.2)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    # Output layer for binary classification\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Print Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
